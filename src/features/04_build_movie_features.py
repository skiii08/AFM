# src/features/04_build_movie_features.py (æœ€çµ‚ä¿®æ­£ç‰ˆ - KeyErrorå¯¾å¿œæ¸ˆã¿)

import torch
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import List, Tuple, Dict
import re
from collections import Counter
from sklearn.decomposition import PCA
import torch as th  # torch.catã¨ã®ç«¶åˆã‚’é¿ã‘ã‚‹ãŸã‚thã¨ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¿…é ˆ)
try:
    import fasttext
except ImportError:
    fasttext = None
    print("[ERROR] fasttext library not found. Please run: pip install fasttext")

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None
    print("[ERROR] sentence-transformers library not found. Please run: pip install sentence-transformers")

# === ãƒ‘ã‚¹è¨­å®š ===
# å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«(04) -> features(1) -> src(2) -> AFM(3)
PROJECT_ROOT = Path(__file__).resolve().parents[2]
DATA_RAW = PROJECT_ROOT / "data/raw"
DATA_PROCESSED = PROJECT_ROOT / "data/processed"
FEATURES_DIR = DATA_PROCESSED / "features"

# å…¥åŠ›
MOVIES_CSV = DATA_RAW / "movies_metadata.csv"

# ã€ğŸ”‘è¦ä¿®æ­£ã€‘FastTextãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãã ã•ã„
FASTTEXT_MODEL_PATH = PROJECT_ROOT / "data/external/cc.en.300.bin"

# å‡ºåŠ›
MOVIE_FEATURES_PT = DATA_PROCESSED / "movie_features.pt"
MOVIE_FEATURE_INFO_JSON = FEATURES_DIR / "movie_feature_info.json"


# --- è£œåŠ©é–¢æ•° ---

def parse_list_cell(s: str) -> list[str]:
    """åŒºåˆ‡ã‚Šæ–‡å­—(;)ã§ãƒªã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹"""
    if pd.isna(s) or s == "":
        return []
    return [x.strip() for x in str(s).split(";") if x.strip()]


def zscore_with_missing(series: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚·ãƒªãƒ¼ã‚ºã‚’Z-Scoreæ­£è¦åŒ–ã—ã€æ¬ æå€¤ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã‚‚è¿”ã™ã€‚
    æ¬ æå€¤ã¯å¹³å‡å€¤ã§è£œå®Œã•ã‚Œã‚‹ã€‚
    """
    not_na = series.notna()
    data = series[not_na].values.astype(np.float32)

    if data.size == 0:
        return np.zeros_like(series, dtype=np.float32), np.zeros_like(series, dtype=np.float32)

    # Z-Scoreè¨ˆç®—
    mean_val = data.mean()
    std_val = data.std()

    if std_val == 0:
        zscore_data = np.zeros_like(data, dtype=np.float32)
    else:
        zscore_data = (data - mean_val) / std_val

    # å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹Z-Scoreé…åˆ—ã‚’ä½œæˆ
    zscore_full = np.zeros_like(series, dtype=np.float32)
    zscore_full[not_na] = zscore_data

    # æ¬ æå€¤ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿
    missing_indicator = (~not_na).values.astype(np.float32)

    return zscore_full, missing_indicator


def keywords_to_hybrid_vec(ft_model: fasttext.FastText._FastText, sbert_model: SentenceTransformer,
                           kws: List[str], kw_counter: Counter, pca: PCA) -> np.ndarray:
    """
    FastText, SBERT, PCAã‚’çµ„ã¿åˆã‚ã›ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    æ¬¡å…ƒ: 300D (FTå¹³å‡) + 1D (ç·æ•°) + 64D (SBERT PCA) = 365D
    """
    if not kws:
        return np.zeros(365, dtype=np.float32)

    # 1. FastText (FT) å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« (300D)
    ft_vecs = [ft_model.get_word_vector(kw) for kw in kws if kw in ft_model]
    if ft_vecs:
        ft_mean_vec = np.mean(ft_vecs, axis=0)
    else:
        ft_mean_vec = np.zeros(300, dtype=np.float32)

    # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç·æ•° (1D)
    num_kws = np.array([len(kws)], dtype=np.float32)

    # 3. SBERT PCA (64D) - é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’è€ƒæ…®
    sbert_vecs = sbert_model.encode(kws)
    sbert_mean_vec = np.mean(sbert_vecs, axis=0).reshape(1, -1)
    sbert_pca_vec = pca.transform(sbert_mean_vec).flatten()  # 64D

    # çµåˆ (300D + 1D + 64D = 365D)
    hybrid_vec = np.concatenate([ft_mean_vec, num_kws, sbert_pca_vec])

    return hybrid_vec


def main():
    print("=" * 80)
    print("4. BUILD MOVIE FEATURES (Creating movie_features.pt - Real Data Only)")
    print("=" * 80)

    DATA_PROCESSED.mkdir(parents=True, exist_ok=True)

    # 1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†
    try:
        movies_df = pd.read_csv(MOVIES_CSV)
        # movie_idã‚’æ•´æ•°ã«å¤‰æ›
        movies_df['movie_id'] = pd.to_numeric(movies_df['movie_id'], errors='coerce').fillna(-1).astype(int)
        movies_df = movies_df[movies_df['movie_id'] != -1].copy()
        movies_df.sort_values(by='movie_id', inplace=True)
        movies_df.reset_index(drop=True, inplace=True)
        num_movies = len(movies_df)
        print(f"[Preproc] Converted movie_id to integer and kept {num_movies} movies.")
    except FileNotFoundError:
        print(f"[ERROR] Movies metadata file not found at {MOVIES_CSV}. Aborting.")
        return

    print(f"[Load] Loaded {num_movies} movies from {MOVIES_CSV.name}")

    # 2. çµ±è¨ˆç‰¹å¾´é‡ (8D)
    # ğŸ’¥ ä¿®æ­£: å®Ÿéš›ã®ã‚«ãƒ©ãƒ åã«åˆã‚ã›ã¦ä¿®æ­£
    STATS_COLS = ['runtime', 'rating', 'num_raters', 'num_reviews']
    stats_features_list = []

    print("[Feature] Generating Statistical Features (runtime, rating, num_raters, num_reviews)...")
    for col in STATS_COLS:
        # KeyErrorã®åŸå› ç®‡æ‰€
        zscore, missing_ind = zscore_with_missing(movies_df[col])
        stats_features_list.append(torch.tensor(zscore, dtype=torch.float32).unsqueeze(1))
        stats_features_list.append(torch.tensor(missing_ind, dtype=torch.float32).unsqueeze(1))

    stats_features = torch.cat(stats_features_list, dim=1)
    print(f"  Statistical Feature Shape: {stats_features.shape} ({stats_features.shape[1]}D)")  # 8D

    # 3. å±æ€§ç‰¹å¾´é‡ (Genres, Production, Cast/Crew, etc. - 263D)

    # 3. å±æ€§ç‰¹å¾´é‡ (Genres, Production, Cast/Crew, etc. - 263D)

    # ğŸ’¥ ä¿®æ­£: å±æ€§ç‰¹å¾´é‡ã‚’æ˜ç¤ºçš„ã«æŠ½å‡ºã™ã‚‹
    EXCLUDE_COLS = [
        'movie_id', 'movie_title', 'genres', 'release_date', 'production_countries',
        'runtime', 'original_language', 'spoken_languages', 'directors', 'actors',
        'keywords', 'rating', 'num_raters', 'num_reviews'
    ]

    # é™¤å¤–ãƒªã‚¹ãƒˆã«ãªã„ã€æ®‹ã‚Šã®ã‚«ãƒ©ãƒ ï¼ˆA01ï½V52ã®ã‚ˆã†ãªç‰¹å¾´é‡ã‚«ãƒ©ãƒ ï¼‰ã‚’å±æ€§ç‰¹å¾´é‡ã¨ã™ã‚‹
    # 3. å±æ€§ç‰¹å¾´é‡ (A01ã€œV52) ã®æŠ½å‡ºã¨æ­£è¦åŒ– ğŸ’¥ã€ä¿®æ­£ç®‡æ‰€ã€‘
    # 04_build_movie_features.py ã®å±æ€§ç‰¹å¾´é‡ï¼ˆattribute_colsï¼‰å‡¦ç†éƒ¨åˆ†ã®ä¿®æ­£

    # 3. å±æ€§ç‰¹å¾´é‡ (A01ã€œV52) ã®æŠ½å‡ºã¨æ­£è¦åŒ– ğŸ’¥ã€ä¿®æ­£ç®‡æ‰€ã€‘
    # 3. å±æ€§ç‰¹å¾´é‡ (A01ã€œV52) ã®æŠ½å‡ºã¨æ­£è¦åŒ– ğŸ’¥ã€ä¿®æ­£ç®‡æ‰€ã€‘
    attribute_cols = [c for c in movies_df.columns if c not in EXCLUDE_COLS]

    normalized_attributes = []
    transform_applied = 0
    max_z_scores = {}

    print("  Applying Sqrt-transform and Z-Score normalization to 263 Attribute features...")

    for col in tqdm(attribute_cols, desc="Normalizing attributes"):
        series = movies_df[col]

        # æ¬ æå€¤ã¯0ã¨ã—ã¦æ‰±ã„ã€æ•°å€¤å‹ã«å¤‰æ›
        series = pd.to_numeric(series, errors='coerce').fillna(0)

        # ã€å¤‰æ›´ã€‘Log(1+x)ã‹ã‚‰Sqrt(å¹³æ–¹æ ¹)å¤‰æ›ã¸åˆ‡ã‚Šæ›¿ãˆ
        if (series < 0).any():
            transformed = series
        else:
            # åã‚Šã‚’å¼·åŠ›ã«æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã®å¹³æ–¹æ ¹å¤‰æ›
            transformed = np.sqrt(series)
            transform_applied += 1

        # Z-Scoreã‚’è¨ˆç®—
        z_score, _ = zscore_with_missing(transformed)

        # æ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯: æ­£è¦åŒ–å¾Œã®æœ€å¤§å€¤ã‚’è¨˜éŒ²
        if z_score.size > 0:
            max_val = np.max(z_score)
            max_z_scores[col] = max_val

        normalized_attributes.append(z_score[:, np.newaxis])

    # çµåˆã—ã¦å±æ€§ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    attribute_features = torch.tensor(
        np.hstack(normalized_attributes),
        dtype=torch.float32
    )

    # 3.5. ã€è¿½åŠ ã€‘å±æ€§ç‰¹å¾´é‡ãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã«L2æ­£è¦åŒ–ã‚’é©ç”¨
    epsilon = 1e-6
    norm = torch.linalg.norm(attribute_features, dim=1, keepdim=True) + epsilon
    attribute_features_final = attribute_features / norm
    print(f"  Applied L2-Normalization to Attribute features (263D).")

    # ğŸ’¥ã€L2æ­£è¦åŒ–ã®æˆåŠŸãƒ‡ãƒãƒƒã‚°ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ã€‘
    # L2ãƒãƒ«ãƒ ãŒ1ã«åæŸã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼ã™ã‚‹
    final_norm = torch.linalg.norm(attribute_features_final, dim=1)
    max_value_after_l2 = torch.max(attribute_features_final).item()

    print("\n" + "=" * 60)
    print("ã€L2æ­£è¦åŒ– æˆåŠŸæ¤œè¨¼ã€‘")
    print("-" * 60)

    # 1. L2ãƒãƒ«ãƒ ã®æ¤œè¨¼ (é•·ã•ãŒ1ã«ãªã£ã¦ã„ã‚‹ã‹)
    print(f"âœ… L2ãƒãƒ«ãƒ ã®å¹³å‡: {final_norm.mean().item():.6f} (ç›®æ¨™: 1.000000)")
    print(f"âœ… L2ãƒãƒ«ãƒ ã®æ¨™æº–åå·®: {final_norm.std().item():.6f} (ç›®æ¨™: 0.000000)")

    # 2. å€¤ã®ç¯„å›²ã®æ¤œè¨¼ (æœ€å¤§å€¤ãŒ1.0ã‚’è¶…ãˆã¦ã„ãªã„ã‹)
    print(f"âœ… æ­£è¦åŒ–å¾Œã®æœ€å¤§å€¤: {max_value_after_l2:.6f} (åˆ¶ç´„: <= 1.0)")

    if final_norm.mean().item() > 0.999 and final_norm.std().item() < 1e-5:
        print("ğŸŒŸ L2æ­£è¦åŒ–ã¯å®Œå…¨ã«æˆåŠŸã—ã¦ãŠã‚Šã€ã™ã¹ã¦ã®æ˜ ç”»ãƒãƒ¼ãƒ‰ã®å±æ€§ç‰¹å¾´é‡ã¯å‡è¡¡ãŒå–ã‚Œã¦ã„ã¾ã™ã€‚")
        print("   â†’ **ã“ã®æ™‚ç‚¹ã§ã€ã‚¿ã‚°ã®ã€Œæ•°ãŒå¤šã„ã“ã¨ã«ã‚ˆã‚‹å¯„ä¸ã®åã‚Šã€ã¯è§£æ¶ˆã•ã‚Œã¦ã„ã¾ã™ã€‚**")
    else:
        print("âŒ L2æ­£è¦åŒ–ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ğŸ’¥ã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ãƒ­ã‚¸ãƒƒã‚¯ - å®Ÿè¡Œçµæœå‡ºåŠ›ç”¨ã€‘
    if max_z_scores:
        max_col = max(max_z_scores, key=max_z_scores.get)
        max_z = max_z_scores[max_col]

        high_z_count = sum(1 for z in max_z_scores.values() if z > 5.0)

        print("\n" + "=" * 60)
        print("ã€å±æ€§ç‰¹å¾´é‡ (A01ã€œV52) å‡è¡¡æ¤œè¨¼çµæœ - Sqrt + Z-Scoreé©ç”¨å¾Œã€‘")
        print(f"ğŸ“ˆ æœ€ã‚‚åã‚ŠãŒæ®‹ã‚‹ã‚«ãƒ©ãƒ : {max_col} (Max Z-Score: {max_z:.4f})")
        print(f"âš  Z-Score > 5.0 ã¨ãªã‚‹ã‚«ãƒ©ãƒ æ•°: {high_z_count} / {len(max_z_scores)}")
        print("âœ… **L2æ­£è¦åŒ–ã‚’é©ç”¨ã—ãŸãŸã‚ã€ã“ã®Z-Scoreå€¤ã¯å‚è€ƒæƒ…å ±ã¨ãªã‚Šã¾ã™ã€‚**")
        print("   â†’ L2æ­£è¦åŒ–ã«ã‚ˆã‚Šã€å„æ˜ ç”»ã®å±æ€§ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ãŒå¼·åˆ¶çš„ã«å‡è¡¡åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚")
        print("=" * 60)

    # 4. æœ€çµ‚ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ«ã®çµåˆï¼ˆçµåˆé †åºã‚’ä¿®æ­£ï¼‰


    print("[Feature] Extracting all Attribute Features (A**, C**, D**, E**, J**, P**, S**, T**, V**)...")
    print(f"  Attribute Feature Shape: {attribute_features.shape} ({attribute_features.shape[1]}D)")

    # =========================================================================
    # ğŸ’¥ 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç‰¹å¾´é‡ (Hybrid) ã®ç”Ÿæˆ (365D)
    # =========================================================================

    if fasttext is None or SentenceTransformer is None:
        print("\n[SKIP] Keyword features skipped due to missing libraries (fasttext or sentence-transformers).")
        keyword_features = torch.zeros((num_movies, 365), dtype=torch.float32)
        kw_dim = 365
    else:
        print("\n[Feature] Generating Keyword Hybrid Features (365D)...")
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        ft_model = None
        sbert_model = None
        try:
            ft_model = fasttext.load_model(str(FASTTEXT_MODEL_PATH))
            print("  [FastText] Model loaded successfully.")
        except Exception as e:
            print(f"  [ERROR] FastText model failed to load. Path: {FASTTEXT_MODEL_PATH}. Error: {e}")

        if ft_model and SentenceTransformer is not None:
            try:
                print("  [SBERT] Loading all-MiniLM-L6-v2...")
                sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception as e:
                print(f"  [ERROR] SBERT model failed to load. Error: {e}")
                sbert_model = None

        if ft_model and sbert_model:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‘ãƒ¼ã‚¹
            movies_df["keywords"] = movies_df["keywords"].fillna("").astype(str)
            kw_lists = movies_df["keywords"].apply(parse_list_cell).tolist()

            # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
            kw_counter = Counter()
            for kws in kw_lists:
                kw_counter.update(kws)

            # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Count >= 5)
            freq_kws = [kw for kw, cnt in kw_counter.items() if cnt >= 5]
            print(f"  [Keywords] Total: {len(kw_counter)}, Freq>=5: {len(freq_kws)}")

            # PCAå­¦ç¿’ (SBERTãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨)
            print("  [PCA] Training on frequent keywords...")
            if len(freq_kws) > 0:
                sbert_freq_vecs = sbert_model.encode(freq_kws, show_progress_bar=False)
                pca = PCA(n_components=64)  # 64Dã«å‰Šæ¸›
                pca.fit(sbert_freq_vecs)
            else:
                print("  [WARNING] No frequent keywords found. Skipping PCA and using zero vector.")
                pca = None

            # Hybrid vectorsã®ç”Ÿæˆ
            print("  [Hybrid] Generating keyword vectors...")
            if pca is not None:
                kw_vecs = [keywords_to_hybrid_vec(ft_model, sbert_model, kws, kw_counter, pca)
                           for kws in tqdm(kw_lists, desc="Generating Hybrid Feats")]
                kw_vecs = np.stack(kw_vecs, axis=0)
                kw_dim = kw_vecs.shape[1]  # 365D
                keyword_features = torch.tensor(kw_vecs, dtype=torch.float32)
            else:
                keyword_features = torch.zeros((num_movies, 365), dtype=torch.float32)
                kw_dim = 365
                print("  [WARNING] PCA setup failed. Keyword features set to zero vectors (365D).")
        else:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚
            keyword_features = torch.zeros((num_movies, 365), dtype=torch.float32)
            kw_dim = 365
            print("  [WARNING] Model(s) failed to load. Keyword features set to zero vectors (365D).")

        print(f"  Keyword Feature Shape: {keyword_features.shape} ({kw_dim}D)")

    # 5. æœ€çµ‚ç‰¹å¾´é‡ã®çµåˆ
    print("\n[Combine] Combining final Movie features (Stats + Attributes + Keywords)...")

    # çµåˆé †åº: STATS_FEATURES (8D) + ATTRIBUTE_FEATURES (263D) + KEYWORD_FEATURES (365D)


    movie_features_final = torch.cat([
        stats_features,  # 8D
        attribute_features_final,  # 263D ã€ä¿®æ­£ã€‘L2æ­£è¦åŒ–å¾Œã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨
        keyword_features  # 365D
    ], dim=1)

    final_dim = movie_features_final.shape[1]

    print(f"  Final shape: {movie_features_final.shape} (Total {final_dim}D)")

    # 6. ä¿å­˜
    print("\n[Save] Saving features...")
    th.save(movie_features_final, MOVIE_FEATURES_PT)
    print(f"  Features saved to: {MOVIE_FEATURES_PT}")

    # 7. å¿…é ˆ: æ˜ ç”»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ± (05_build_graph.pyã§å¿…è¦)
    movie_id_to_idx = {
        row['movie_id']: idx
        for idx, row in movies_df.reset_index().rename(columns={'index': 'movie_idx'}).iterrows()
    }
    movie_ids = movies_df['movie_id'].tolist()

    # ğŸ’¥ å¿…é ˆ: ç‰¹å¾´é‡æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ªãƒ•ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å«ã‚ã‚‹
    stats_end = stats_features.shape[1]
    attribute_end = stats_end + attribute_features.shape[1]

    movie_feature_info = {
        "num_movies": num_movies,
        "movie_feat_dim": final_dim,
        "movie_ids": movie_ids,
        "offsets": {
            "stats": {"start": 0, "end": stats_end, "dim": stats_features.shape[1]},  # 8D
            "attribute": {"start": stats_end,
                          "end": attribute_end,
                          "dim": attribute_features.shape[1]},  # 263D
            "keyword_hybrid": {"start": attribute_end,
                               "end": final_dim,
                               "dim": kw_dim}  # 365D
        }
    }

    with open(MOVIE_FEATURE_INFO_JSON, 'w') as f:
        json.dump(movie_feature_info, f, indent=2)
    print(f"  Saved movie index info: {MOVIE_FEATURE_INFO_JSON}")


if __name__ == "__main__":
    try:
        from tqdm import tqdm
    except ImportError:
        def tqdm(iterable, *args, **kwargs):
            return iterable

    if fasttext is None or SentenceTransformer is None:
        print("\n[ABORT] Cannot run due to missing required libraries (fasttext or sentence-transformers).")
        # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãªã„å ´åˆã¯å¼·åˆ¶çµ‚äº†
        # sys.exit(1) ã¯ã€ä»Šå›ã¯ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã«ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§ç¶™ç¶šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¾ã™ã€‚
        # ãŸã ã—ã€ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã«ã‚‚ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§365DãŒä¿è¨¼ã•ã‚Œã‚‹ã‚ˆã†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ã—ã¾ã—ãŸã€‚
        pass

    main()