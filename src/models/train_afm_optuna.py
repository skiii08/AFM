# src/models/train_afm_optuna.py (Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ©æ¢ç´¢ç‰ˆ)

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from pathlib import Path
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import random
import os
import optuna  # ğŸ’¥ æ–°è¦: Optunaã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ


# ===================================================================
# 1. å®‰å®šåŒ–æªç½®: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã®å›ºå®š
# ===================================================================

def set_seed(seed_value=42):
    """ã™ã¹ã¦ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã—ã€å­¦ç¿’ã®å†ç¾æ€§ã‚’ç¢ºä¿ã—ã¾ã™ã€‚"""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)

    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed_value)


# ===================================================================
# 2. AFM MODEL DEFINITION (Z-Scoreäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«) - train_afm.pyã‹ã‚‰ãã®ã¾ã¾æµç”¨
# ===================================================================
class AFM(nn.Module):
    def __init__(self, user_dim, movie_dim, edge_dim,
                 embedding_dim=32,
                 attn_size=32,
                 dropout_rate=0.2):
        super().__init__()

        self.user_embed = nn.Linear(user_dim, embedding_dim, bias=False)
        self.movie_embed = nn.Linear(movie_dim, embedding_dim, bias=False)

        attn_input_dim = embedding_dim + edge_dim
        self.attn_layer = nn.Sequential(
            nn.Linear(attn_input_dim, attn_size),
            nn.ReLU(),
            nn.Linear(attn_size, 1, bias=False)
        )
        self.dropout = nn.Dropout(dropout_rate)

        self.bias = nn.Parameter(torch.zeros(1))
        self.linear_u = nn.Linear(user_dim, 1, bias=False)
        self.linear_m = nn.Linear(movie_dim, 1, bias=False)

    def forward(self, u_feat, m_feat, e_attr):
        v_u = self.user_embed(u_feat)
        v_m = self.movie_embed(m_feat)

        interaction_term = v_u * v_m
        attn_input = torch.cat([interaction_term, e_attr], dim=-1)

        attn_score = self.attn_layer(attn_input)

        weighted_interaction = interaction_term * attn_score
        weighted_interaction = self.dropout(weighted_interaction)
        interaction_sum = torch.sum(weighted_interaction, dim=1)

        linear_term = self.bias + self.linear_u(u_feat).squeeze(1) + self.linear_m(m_feat).squeeze(1)

        prediction = linear_term + interaction_sum

        return prediction


# ===================================================================
# 3. TRAIN HELPER FUNCTIONS
# ===================================================================

# === ãƒ‘ã‚¹ã¨å›ºå®šè¨­å®š ===
PROJECT_ROOT = Path("/Users/watanabesaki/PycharmProjects/AFM")  # ğŸ’¥ è‡ªèº«ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«åˆã‚ã›ã¦ãã ã•ã„
DATA_PROCESSED = PROJECT_ROOT / "data/processed"
MODELS_DIR = PROJECT_ROOT / "models"
MODELS_DIR.mkdir(exist_ok=True)

TRAIN_PT = DATA_PROCESSED / "hetero_graph_train.pt"
VAL_PT = DATA_PROCESSED / "hetero_graph_val.pt"
# æ¢ç´¢æ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‹ã€ãƒ™ã‚¹ãƒˆãªã‚‚ã®ã ã‘ä¿å­˜ã—ã¾ã™
MODEL_SAVE_PATH = MODELS_DIR / "afm_model_optuna_best.pt"

# å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
NUM_EPOCHS = 100  # æ¢ç´¢æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ã€ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ¸›ã‚‰ã™ã“ã¨ã‚’æ¨å¥¨
BATCH_SIZE = 512
DROPOUT_RATE = 0.2

# ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¨ãƒƒã‚¸ç‰¹å¾´é‡ã®ãƒªãƒ¼ã‚¯æ¬¡å…ƒ
USER_MEAN_ABS_INDEX = 26
USER_STD_ABS_INDEX = 27
EDGE_LEAK_START_INDEX = 36  # Dim 36, 37 ã‚’å‰Šé™¤


def load_and_preprocess_data(path: Path):
    """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€AFMå½¢å¼ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã€‚ãƒ‡ãƒãƒ¼ãƒãƒ©ã‚¤ã‚ºã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚æŠ½å‡ºã€‚"""
    try:
        data = torch.load(path, weights_only=False)
    except FileNotFoundError:
        print(f"[ERROR] Data not found at {path}. Aborting.")
        return None, None, None, None, None, None, None

    u_feat_all = data['user'].x  # 200D
    m_feat_all = data['movie'].x

    # ãƒ‡ãƒãƒ¼ãƒãƒ©ã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’çµ¶å¯¾ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰æŠ½å‡º
    user_mean_rating = u_feat_all[:, USER_MEAN_ABS_INDEX]
    user_std_rating = u_feat_all[:, USER_STD_ABS_INDEX]
    user_std_rating = torch.where(user_std_rating == 0, torch.tensor(1.0, dtype=torch.float32), user_std_rating)

    edge_data = data['user', 'review', 'movie']
    u_indices, m_indices = edge_data.edge_index
    e_attr_full = edge_data.edge_attr  # 38D
    y_norm = edge_data.y  # Z-Scoreã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

    # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡ã‹ã‚‰Î¼ã¨Ïƒã‚’å‰Šé™¤ (200D -> 198D)
    features_before_mu = u_feat_all[:, :USER_MEAN_ABS_INDEX]
    features_after_sigma = u_feat_all[:, USER_STD_ABS_INDEX + 1:]
    u_feat_sliced_all = torch.cat([features_before_mu, features_after_sigma], dim=1)  # 198D
    u_feat_indexed = u_feat_sliced_all[u_indices]

    # 2. ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰¹å¾´é‡ã‹ã‚‰ãƒªãƒ¼ã‚¯æ¬¡å…ƒ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹36, 37) ã‚’å‰Šé™¤ (38D -> 36D)
    e_attr_sliced = e_attr_full[:, :EDGE_LEAK_START_INDEX]

    m_feat_indexed = m_feat_all[m_indices]

    # ãƒ‡ãƒãƒ¼ãƒãƒ©ã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¨ãƒƒã‚¸ã”ã¨ã«å–å¾—
    u_mean_indexed = user_mean_rating[u_indices]
    u_std_indexed = user_std_rating[u_indices]

    # Raw Ratingã®çœŸã®å€¤ (Y) ã‚’è¨ˆç®— - è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§é‡ã¿ä»˜ã‘ã«ä½¿ç”¨
    y_raw = (y_norm * u_std_indexed) + u_mean_indexed

    return u_feat_indexed, m_feat_indexed, e_attr_sliced, y_norm, y_raw, u_mean_indexed, u_std_indexed


# Raw Rating ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®è©•ä¾¡é–¢æ•°
def evaluate_model(model, u_feat, m_feat, e_attr, y_true_raw, u_mean, u_std):
    """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®MAEã¨RMSEã‚’è¨ˆç®— (Raw Ratingã‚¹ã‚±ãƒ¼ãƒ«)"""
    model.eval()
    with torch.no_grad():
        # äºˆæ¸¬å€¤ã¯Z-Scoreã‚¹ã‚±ãƒ¼ãƒ«
        y_pred_norm = model(u_feat, m_feat, e_attr)

        # Z-Scoreäºˆæ¸¬å€¤ã‚’ãƒ‡ãƒãƒ¼ãƒãƒ©ã‚¤ã‚º
        y_pred_raw = (y_pred_norm * u_std) + u_mean

        # Raw Ratingã®ç¯„å›²ã«Clamp (ãƒ¢ãƒ‡ãƒ«å¤–éƒ¨ã§é©ç”¨)
        y_pred_raw = torch.clamp(y_pred_raw, min=1.0, max=10.0)
        y_true_raw = torch.clamp(y_true_raw, min=1.0, max=10.0)

        # RMSE/MAE ã¯ Raw Ratingã®èª¤å·®ã§è¨ˆç®—
        rmse = torch.sqrt(F.mse_loss(y_pred_raw, y_true_raw)).item()
        mae = F.l1_loss(y_pred_raw, y_true_raw).item()

    return rmse, mae


# ===================================================================
# 4. OPTUNA OBJECTIVE FUNCTION (æ¢ç´¢å¯¾è±¡ã®é–¢æ•°)
# ===================================================================

def objective(trial: optuna.Trial):
    """
    OptunaãŒæœ€é©åŒ–ã™ã‚‹ç›®çš„é–¢æ•°ã€‚æ¤œè¨¼RMSEã‚’è¿”ã™ã€‚
    """
    set_seed(42)  # ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã”ã¨ã®å†ç¾æ€§ã‚’ç¢ºä¿

    # 1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    # ğŸ’¥ ä¿®æ­£: Optunaã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ‘ãƒ©ã‚’å®šç¾©
    K_EMBED = trial.suggest_categorical('K_EMBED', [32, 64, 128])  # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    ATTN_SIZE = trial.suggest_categorical('ATTN_SIZE', [32, 64, 128])  # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³éš ã‚Œå±¤ã‚µã‚¤ã‚º
    LEARNING_RATE = trial.suggest_float('LEARNING_RATE', 1e-5, 1e-3, log=True)  # å­¦ç¿’ç‡ (å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«)
    LOW_RATING_WEIGHT = trial.suggest_float('LOW_RATING_WEIGHT', 1.0, 30.0)  # ä½è©•ä¾¡é‡ã¿

    print("-" * 80)
    print(
        f"Trial {trial.number}: K_EMBED={K_EMBED}, ATTN_SIZE={ATTN_SIZE}, LR={LEARNING_RATE:.6f}, WEIGHT={LOW_RATING_WEIGHT:.2f}")
    print("-" * 80)

    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ (å…±é€š)
    u_feat_train, m_feat_train, e_attr_train, y_train_norm, y_train_raw, _, _ = load_and_preprocess_data(TRAIN_PT)
    u_feat_val_tmp, m_feat_val_tmp, e_attr_val_tmp, y_val_norm, y_val_raw, u_mean_val, u_std_val = load_and_preprocess_data(
        VAL_PT)

    if u_feat_train is None or u_feat_val_tmp is None:
        raise FileNotFoundError("Training or validation data not found.")

    # 3. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    user_dim = u_feat_train.shape[1]
    movie_dim = m_feat_train.shape[1]
    edge_dim = e_attr_train.shape[1]

    model = AFM(user_dim, movie_dim, edge_dim, embedding_dim=K_EMBED, attn_size=ATTN_SIZE, dropout_rate=DROPOUT_RATE)

    # 4. è¨“ç·´è¨­å®š
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)

    # 5. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    train_dataset = TensorDataset(u_feat_train, m_feat_train, e_attr_train, y_train_norm, y_train_raw)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    # 6. è¨“ç·´ãƒ«ãƒ¼ãƒ—
    best_val_rmse = float('inf')

    for epoch in range(NUM_EPOCHS):
        model.train()

        # æ—©æœŸçµ‚äº† (Pruning) ã‚’å°å…¥
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

        for u_feat, m_feat, e_attr, y_true_norm, y_true_raw in train_loader:
            optimizer.zero_grad()
            y_pred_norm = model(u_feat, m_feat, e_attr)

            # é‡ã¿ä»˜ãæå¤±ã®è¨ˆç®—
            low_rating_mask = y_true_raw <= 4.0
            weights = torch.ones_like(y_true_raw, dtype=torch.float32)
            weights[low_rating_mask] = LOW_RATING_WEIGHT
            squared_error = (y_pred_norm - y_true_norm) ** 2
            loss = (weights * squared_error).mean()

            loss.backward()
            optimizer.step()

        # æ¤œè¨¼
        val_rmse, _ = evaluate_model(model, u_feat_val_tmp, m_feat_val_tmp, e_attr_val_tmp, y_val_raw, u_mean_val,
                                     u_std_val)

        # Optunaã«ç¾åœ¨ã®ã‚¹ã‚³ã‚¢ã‚’å ±å‘Š
        trial.report(val_rmse, epoch)

        if val_rmse < best_val_rmse:
            best_val_rmse = val_rmse
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã ãŒã€ã“ã“ã§ã¯é€Ÿåº¦å„ªå…ˆã§ã‚¹ã‚­ãƒƒãƒ—

    # 7. æœ€çµ‚çš„ãªæœ€å°æ¤œè¨¼RMSEã‚’è¿”ã™ (Optunaã®ç›®çš„ã¯æœ€å°åŒ–)
    return best_val_rmse


# ===================================================================
# 5. OPTUNA STUDY MAIN
# ===================================================================

def main_optuna():
    set_seed(42)
    print("=" * 80)
    print("AFM OPTUNA HYPERPARAMETER OPTIMIZATION")
    print("=" * 80)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’è¨­å®šã—ã¦çµæœã‚’æ°¸ç¶šåŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ (ä¾‹: sqlite:///db.sqlite3)
    # æ°¸ç¶šåŒ–ã—ãªã„å ´åˆã¯ `storage=None` ã§ãƒ¡ãƒ¢ãƒªä¸Šã§å®Ÿè¡Œã•ã‚Œã¾ã™
    study = optuna.create_study(
        direction='minimize',  # æœ€å°åŒ–ã™ã‚‹æŒ‡æ¨™ (RMSE)
        sampler=optuna.samplers.TPESampler(seed=42),  # æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)  # æ—©æœŸçµ‚äº†ã®è¨­å®š
    )

    # æ¢ç´¢ã®å®Ÿè¡Œ
    # n_trials: è©¦è¡Œå›æ•° (æ¢ç´¢ã®æ·±ã•/åºƒã•)
    study.optimize(objective, n_trials=50, show_progress_bar=True)

    # çµæœã®è¡¨ç¤º
    print("\n[Complete] Optimization finished.")
    print("-" * 60)
    print(f"Best Trial (RMSE={study.best_value:.4f})")
    print("-" * 60)
    print("Best Hyperparameters:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    # æœ€é©ãªãƒã‚¤ãƒ‘ãƒ©ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ãƒ»ä¿å­˜ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™

    # ğŸ’¥ ãƒ™ã‚¹ãƒˆãªãƒã‚¤ãƒ‘ãƒ©ã‚’ `train_afm.py` ã«åæ˜ ã•ã›ã¦ã€æ”¹ã‚ã¦ã‚¨ãƒãƒƒã‚¯ã‚’å¢—ã‚„ã—ã¦å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚


if __name__ == "__main__":
    main_optuna()